{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamrishigandhi/4301-Lab-1/blob/main/Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjGPtk48jLT7"
      },
      "source": [
        "# Computer Vision Lab 2\n",
        "\n",
        "Welcome friends, it's time for Deep Learning with PyTorch! This homework might need a longer running time.\n",
        "Keep this in mind and start early.\n",
        "\n",
        "PyTorch is a deep learning framework for fast, flexible experimentation. We are going to use it to train our classifiers.\n",
        "\n",
        "For this homework you need to turn in this file `Lab2.ipynb` after running your results and answering questions in-line.\n",
        "\n",
        "**Notes**:\n",
        " - This assignment was designed to be used with Google Colab, but feel free to set up your own environment if you wish. Just bear in mind that we cannot provide support for custom environments.\n",
        " - Feel free to create new cells as needed, but please **do not delete existing cells**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GGQCcvq84eP"
      },
      "source": [
        "Before you get started, we suggest you do the [PyTorch tutorial first](https://github.com/param087/Pytorch-tutorial-on-Google-colab).\n",
        "\n",
        "You should at least do the 60 Minute Blitz up until \"Training a Classifier\".\n",
        "\n",
        "**How to use this notebook:**\n",
        " - Each cell with a grey background is executable.\n",
        " - They can be executed by pressing the \"Play\" button or by hitting `Shift+Enter`\n",
        " - Cells can be executed out of order.\n",
        " - You can add new cells by clicking on the `+ Code` button in the header.\n",
        " - Made a mistake a need to start over? Click *(Runtime => Restart runtime)*\n",
        " - Check out this [Colab Introduction](https://colab.research.google.com/notebooks/intro.ipynb#scrollTo=5fCEDCU_qrC0) if you're having trouble.\n",
        "\n",
        "\n",
        " This will make sure that your progress will be saved to your Google Drive, and won't be lost if your browser refreshes for some reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytzMI9eMGZcF"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This will set up the environment without GPUs. This is the recommended setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Cn6ci5wb84eQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79d5e94-694f-4258-fd16-44720d55ef13"
      },
      "source": [
        "! pip install torch==1.5.0+cpu torchvision==0.6.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "! pip install tqdm matplotlib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.5.0+cpu (from versions: 1.11.0, 1.11.0+cpu, 1.11.0+cu102, 1.11.0+cu113, 1.11.0+cu115, 1.11.0+rocm4.3.1, 1.11.0+rocm4.5.2, 1.12.0, 1.12.0+cpu, 1.12.0+cu102, 1.12.0+cu113, 1.12.0+cu116, 1.12.0+rocm5.0, 1.12.0+rocm5.1.1, 1.12.1, 1.12.1+cpu, 1.12.1+cu102, 1.12.1+cu113, 1.12.1+cu116, 1.12.1+rocm5.0, 1.12.1+rocm5.1.1, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.0+cu117.with.pypi.cudnn, 1.13.0+rocm5.1.1, 1.13.0+rocm5.2, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 1.13.1+cu117.with.pypi.cudnn, 1.13.1+rocm5.1.1, 1.13.1+rocm5.2, 2.0.0, 2.0.0+cpu, 2.0.0+cpu.cxx11.abi, 2.0.0+cu117, 2.0.0+cu117.with.pypi.cudnn, 2.0.0+cu118, 2.0.0+rocm5.3, 2.0.0+rocm5.4.2, 2.0.1, 2.0.1+cpu, 2.0.1+cpu.cxx11.abi, 2.0.1+cu117, 2.0.1+cu117.with.pypi.cudnn, 2.0.1+cu118, 2.0.1+rocm5.3, 2.0.1+rocm5.4.2, 2.1.0, 2.1.0+cpu, 2.1.0+cpu.cxx11.abi, 2.1.0+cu118, 2.1.0+cu121, 2.1.0+cu121.with.pypi.cudnn, 2.1.0+rocm5.5, 2.1.0+rocm5.6, 2.1.1, 2.1.1+cpu, 2.1.1+cpu.cxx11.abi, 2.1.1+cu118, 2.1.1+cu121, 2.1.1+cu121.with.pypi.cudnn, 2.1.1+rocm5.5, 2.1.1+rocm5.6, 2.1.2, 2.1.2+cpu, 2.1.2+cpu.cxx11.abi, 2.1.2+cu118, 2.1.2+cu121, 2.1.2+cu121.with.pypi.cudnn, 2.1.2+rocm5.5, 2.1.2+rocm5.6, 2.2.0, 2.2.0+cpu, 2.2.0+cpu.cxx11.abi, 2.2.0+cu118, 2.2.0+cu121, 2.2.0+rocm5.6, 2.2.0+rocm5.7, 2.2.1, 2.2.1+cpu, 2.2.1+cpu.cxx11.abi, 2.2.1+cu118, 2.2.1+cu121, 2.2.1+rocm5.6, 2.2.1+rocm5.7)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.5.0+cpu\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF5Xx32A_RJs"
      },
      "source": [
        "# We're not using the GPU.\n",
        "use_gpu = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b37CIwsc84eT"
      },
      "source": [
        "### With GPUs\n",
        "If you're feeling adventurous you can use GPUs to accelerate training. Follow the following steps. Just note that GPUs might not be available. The course staff also can't provide support for GPU-related issues so if you're having trouble please just use the CPU runtime.\n",
        "\n",
        " 1. Go to Runtime > Change runtime type and select 'GPU'\n",
        " 2. Restart the Runtime, uncomment the commands below and run them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xidZAA_bjL1G",
        "scrolled": true
      },
      "source": [
        "# Install the necessary packages\n",
        "\n",
        "# ! pip install torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# ! pip install tqdm matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Yob-Du0qml"
      },
      "source": [
        "If you want to use the GPU, uncomment the line below and run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTRfzE_s84eY"
      },
      "source": [
        "# Uncomment this and execute if you're using the GPU.\n",
        "# use_gpu = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsBJga6584ec"
      },
      "source": [
        "### Check that things are working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy8bYB-YXilT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65503641-6240-4d0f-d441-4080cb46add6"
      },
      "source": [
        "# Make sure things work.\n",
        "\n",
        "import torch\n",
        "\n",
        "if use_gpu:\n",
        "    print(torch.zeros(10).cuda())\n",
        "else:\n",
        "    print(torch.zeros(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5Zp2dQmYIsH"
      },
      "source": [
        " ## Initialize Datasets\n",
        "\n",
        " This code defines the data loaders that will be used to train and test our networks. It also defines data augmentation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ1otUNqX08V",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611bb657-d28f-4f28-f4e3-095beb134253"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "default_train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize rescales and shifts the data so that it has a zero mean\n",
        "    # and unit variance. This reduces bias and makes it easier to learn!\n",
        "    # The values here are the mean and variance of our inputs.\n",
        "    # This will change the input images to be centered at 0 and be\n",
        "    # between -1 and 1.\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "default_test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "\n",
        "def get_train_loader(batch_size, transform=default_train_transform):\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform)\n",
        "    return torch.utils.data.DataLoader(\n",
        "        trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "\n",
        "def get_test_loader(batch_size, transform=default_test_transform):\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform)\n",
        "    return torch.utils.data.DataLoader(\n",
        "        testset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "# This downloads the datasets.\n",
        "get_train_loader(1)\n",
        "get_test_loader(1);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtwZwWdp84ej"
      },
      "source": [
        "## Define code that trains and tests code.\n",
        "\n",
        "This code will train your model. Feel free to read the code below, but we suggest you don't modify it unless you know what you're doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsHmTDLQkBgU"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# The function we'll call to train the network each epoch\n",
        "def train(net, loader, optimizer, criterion, epoch, use_gpu=False):\n",
        "    running_loss = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Send the network to the correct device\n",
        "    if use_gpu:\n",
        "        net = net.cuda()\n",
        "    else:\n",
        "        net = net.cpu()\n",
        "\n",
        "    # tqdm is a useful package for adding a progress bar to your loops\n",
        "    pbar = tqdm(loader)\n",
        "    for i, data in enumerate(pbar):\n",
        "        inputs, labels = data\n",
        "\n",
        "        # If we're using the GPU, send the data to the GPU\n",
        "        if use_gpu:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()  # Set the gradients of the parameters to zero.\n",
        "        outputs = net(inputs)  # Forward pass (send the images through the network)\n",
        "        loss = criterion(outputs, labels)  # Compute the loss w.r.t the labels.\n",
        "        loss.backward()  # Backward pass (compute gradients).\n",
        "        optimizer.step()  # Use the gradients to update the weights of the network.\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_description(f\"[epoch {epoch+1}] loss = {running_loss/(i+1):.03f}\")\n",
        "\n",
        "    average_loss = total_loss / (i + 1)\n",
        "    tqdm.write(f\"Epoch {epoch} summary -- loss = {average_loss:.03f}\")\n",
        "\n",
        "    return average_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ownWKFtri0ZA"
      },
      "source": [
        "This code will evaluate the performance of you network. It won't update the weights, just compute from evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3u1eC-niu6a"
      },
      "source": [
        "from collections import defaultdict\n",
        "from torchvision.utils import make_grid\n",
        "from PIL import Image\n",
        "from IPython import display as ipd\n",
        "\n",
        "\n",
        "def show_hard_negatives(hard_negatives, label, nrow=10):\n",
        "    \"\"\"Visualizes hard negatives\"\"\"\n",
        "    grid = make_grid([(im+1)/2 for im, score in hard_negatives[label]],\n",
        "                     nrow=nrow, padding=1)\n",
        "    grid = grid.permute(1, 2, 0).mul(255).byte().numpy()\n",
        "    ipd.display(Image.fromarray((grid)))\n",
        "\n",
        "\n",
        "# The function we'll call to test the network\n",
        "def test(net, loader, tag='', use_gpu=False, num_hard_negatives=10):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Send the network to the correct device\n",
        "    net = net.cuda() if use_gpu else net.cpu()\n",
        "\n",
        "    # Compute the overall accuracy of the network\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(loader, desc=f\"Evaluating {tag}\"):\n",
        "            images, labels = data\n",
        "\n",
        "            # If we're using the GPU, send the data to the GPU\n",
        "            if use_gpu:\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            # Forward pass (send the images through the network)\n",
        "            outputs = net(images)\n",
        "\n",
        "            # Take the output of the network, and extract the index\n",
        "            # of the largest prediction for each example\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Count the number of correct predictions\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    average_accuracy = correct/total\n",
        "    tqdm.write(f'{tag} accuracy of the network: {100*average_accuracy:.02f}%')\n",
        "\n",
        "    # Repeat above, but estimate the testing accuracy for each of the labels\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "    hard_negatives = defaultdict(list)\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels = data\n",
        "            if use_gpu:\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "            predicted_scores, predicted_labels = torch.max(outputs, 1)\n",
        "            correct_mask = (predicted_labels == labels).squeeze()\n",
        "            incorrect_mask = ~correct_mask\n",
        "            unique_labels, unique_counts = torch.unique(labels, return_counts=True)\n",
        "            for l, c in zip(unique_labels, unique_counts):\n",
        "                l = l.item()\n",
        "                label_mask = (labels == l)\n",
        "                predicted_mask = (predicted_labels == l)\n",
        "                # This keeps track of the most hardest negatives\n",
        "                # i.e. mistakes with the highest confidence.\n",
        "                hard_negative_mask = (~correct_mask & predicted_mask)\n",
        "                if hard_negative_mask.sum() > 0:\n",
        "                    hard_negatives[l].extend([\n",
        "                        (im, score.item())\n",
        "                        for im, score in zip(images[hard_negative_mask],\n",
        "                                             predicted_scores[hard_negative_mask])])\n",
        "                    hard_negatives[l].sort(key=lambda x: x[1], reverse=True)\n",
        "                    hard_negatives[l] = hard_negatives[l][:num_hard_negatives]\n",
        "                class_correct[l] += (correct_mask & label_mask).sum()\n",
        "                class_total[l] += c\n",
        "\n",
        "\n",
        "    for i in range(10):\n",
        "        tqdm.write(f'{tag} accuracy of {classes[i]} = {100*class_correct[i]/class_total[i]:.02f}%')\n",
        "        if len(hard_negatives[i]) > 0:\n",
        "            print(f'Hard negatives for {classes[i]}')\n",
        "            show_hard_negatives(hard_negatives, i, nrow=10)\n",
        "        else:\n",
        "            print(\"There were no hard negatives--perhaps the model got 0% accuracy?\")\n",
        "\n",
        "\n",
        "    return average_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15b1VD7Ui9Qm"
      },
      "source": [
        "This is a wrapper function we provide that handles all the book keeping. It will train your network for an epoch and then test it every couple epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETLv6geWi9Yf"
      },
      "source": [
        "def train_network(net,\n",
        "                  lr,\n",
        "                  epochs,\n",
        "                  batch_size,\n",
        "                  criterion=None,\n",
        "                  lr_func=None,\n",
        "                  train_transform=default_train_transform,\n",
        "                  eval_interval=10,\n",
        "                  use_gpu=use_gpu):\n",
        "    # Initialize the optimizer\n",
        "    # You can change this if you want!\n",
        "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "    # optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    # Initialize the loss function\n",
        "    if criterion is None:\n",
        "        # Note that CrossEntropyLoss has the Softmax built in!\n",
        "        # This is good for numerical stability.\n",
        "        # Read: https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Initialize the data loaders\n",
        "    train_loader = get_train_loader(batch_size, transform=train_transform)\n",
        "    test_loader = get_test_loader(batch_size)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        if lr_func is not None:\n",
        "            lr_func(optimizer, epoch, lr)\n",
        "\n",
        "        train_loss = train(net, train_loader, optimizer, criterion, epoch, use_gpu=use_gpu)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Evaluate the model every `eval_interval` epochs.\n",
        "        if (epoch + 1) % eval_interval == 0:\n",
        "            print(f\"Evaluating epoch {epoch+1}\")\n",
        "            train_accuracy = test(net, train_loader, 'Train', use_gpu=use_gpu)\n",
        "            test_accuracy = test(net, test_loader, 'Test', use_gpu=use_gpu)\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            test_accuracies.append(test_accuracy)\n",
        "\n",
        "    return train_losses, train_accuracies, test_accuracies\n",
        "\n",
        "\n",
        "# A function to plot the losses over time\n",
        "def plot_results(train_losses, train_accuracies, test_accuracies):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axes[0].plot(train_losses)\n",
        "    axes[0].set_title('Training Loss')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "\n",
        "    axes[1].plot(train_accuracies)\n",
        "    axes[1].set_title('Training Accuracy')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "\n",
        "    axes[2].plot(test_accuracies)\n",
        "    axes[2].set_title('Testing Accuracy')\n",
        "    axes[2].set_xlabel('Epoch')\n",
        "    axes[2].set_ylabel('Accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7fg7toAbRJE"
      },
      "source": [
        "\n",
        "## 2.1. Training a classifier using only one fully connected Layer\n",
        "\n",
        "Implement a model to classify the images from Cifar-10 into ten categories using just one fully connected layer (remember that fully connected layers are called Linear in PyTorch).\n",
        "\n",
        "If you are new to PyTorch you may want to check out the tutorial on MNIST [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py).\n",
        "\n",
        "Fill in the code for LazyNet here.\n",
        "\n",
        "**Hints:**\n",
        " - Note that `nn.CrossEntropyLoss` has the Softmax built in for numerical stability. This means that the output layer of your network should be linear and not contain a Softmax. You can read more about it [here](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)\n",
        " - You can use the `view()` function to flatten your input image to a vector e.g., if `x` is a `(100,3,4,4)` tensor then `x.view(-1, 3*4*4)` will flatten it into a vector of size `48`.\n",
        " - The images in MNIST are 32x32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsSqlM8aYB7M"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "class LazyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(32 * 32 * 3, 10)  # Flatten image (32x32x3) to vector and map to 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input image\n",
        "        x = x.view(-1, 32 * 32 * 3)\n",
        "        # Forward pass through the fully connected layer\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "net = LazyNet()\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-2w5lEj84et"
      },
      "source": [
        "#### Run the model for 15 epochs and report the plots and accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHYGGCu-b_gE",
        "scrolled": true
      },
      "source": [
        "train_losses, train_accuracies, test_accuracies = train_network(\n",
        "    net,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    lr=0.01,\n",
        "    epochs=15,\n",
        "    eval_interval=5,\n",
        "    batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlFRsBIi84ew"
      },
      "source": [
        "plot_results(train_losses, train_accuracies, test_accuracies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf1r46NZ84ez"
      },
      "source": [
        "## 2.2. Training a classifier using multiple fully connected layers ##\n",
        "\n",
        "Implement a model for the same classification task using multiple fully connected layers.\n",
        "\n",
        "Start with a fully connected layer that maps the data from image size (32 * 32 * 3) to a vector of size 120, followed by another fully connected that reduces the size to 84 and finally a layer that maps the vector of size 84 to 10 classes.\n",
        "\n",
        "Use any activation you want.\n",
        "\n",
        "Fill in the code for BoringNet below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuoYdwZ9gQq5"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BoringNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 120)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(120, 84)           # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(84, 10)            # Final fully connected layer for classification\n",
        "        self.activation = nn.ReLU()             # ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input image\n",
        "        x = x.view(-1, 32 * 32 * 3)\n",
        "        # Pass through the first fully connected layer followed by activation\n",
        "        x = self.activation(self.fc1(x))\n",
        "        # Pass through the second fully connected layer followed by activation\n",
        "        x = self.activation(self.fc2(x))\n",
        "        # Pass through the final fully connected layer (no activation needed)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = BoringNet()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE4lmp-z84e2"
      },
      "source": [
        "### Run the model for 30 epochs and report the plots and accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RHEbyiy84e3"
      },
      "source": [
        "train_losses, train_accuracies, test_accuracies = train_network(\n",
        "    net,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    lr=0.01,\n",
        "    epochs=30,\n",
        "    batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7gwhaYp84e5"
      },
      "source": [
        "plot_results(train_losses, train_accuracies, test_accuracies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcIXPU9o84e7"
      },
      "source": [
        "### Question\n",
        "\n",
        "Try training this model with and without activations. How does the activations (such as ReLU) affect the training process and why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fha6blZ925uw"
      },
      "source": [
        "[link text](https:// [link text](https://))*Activations, such as ReLU, greatly influence the training process and model performance. With activations, like ReLU, the model can learn complex, nonlinear relationships in the data, aiding in better convergence and higher accuracy. ReLU introduces non-linearities crucial for capturing intricate patterns, preventing the vanishing gradient problem, and facilitating efficient backpropagation. Without activations, the model reduces to a linear function composition, limiting its capacity to learn complex patterns efficiently. This leads to slower convergence, poorer generalization, and a higher risk of the vanishing gradient problem, particularly in deep networks. *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3KpwTHf84e_"
      },
      "source": [
        "\n",
        "## 2.3. Training a classifier using convolutions ##\n",
        "\n",
        "Implement a model using convolutional, pooling and fully connected layers.\n",
        "\n",
        "You are free to choose any parameters for these layers (we would like you to play around with some values).\n",
        "\n",
        "Fill in the code for CoolNet below. Explain why you have chosen these layers and how they affected the performance. Analyze the behavior of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wwU7Lxs84e_"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CoolNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        # Max pooling layers\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # Calculated after two max-pooling layers\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)  # Output size is 10 for 10 classes in CIFAR-10\n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers with ReLU activation and max pooling\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        # Flatten before fully connected layers\n",
        "        x = x.view(-1, 32 * 8 * 8)\n",
        "        # Fully connected layers with ReLU activation\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        # Final output layer (no activation needed)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = CoolNet()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLbwxHnv84fC"
      },
      "source": [
        "### Run the model for 30 epochs and report the plots and accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ZIHJOx_p84fC"
      },
      "source": [
        "net = CoolNet()\n",
        "train_losses, train_accuracies, test_accuracies = train_network(\n",
        "    net,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    lr=0.01,\n",
        "    epochs=30,\n",
        "    batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a47cZGq884fE"
      },
      "source": [
        "plot_results(train_losses, train_accuracies, test_accuracies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdsbz_7p84fH"
      },
      "source": [
        "### 2.3.1. How does batch size affect training?\n",
        "\n",
        "Try using three different values for batch size. How do these values affect training and why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPNcgX0h84fI"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# Define transforms and load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Initialize CoolNet\n",
        "net = CoolNet()\n",
        "\n",
        "# Define different batch sizes\n",
        "batch_sizes = [64, 128, 256]\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    # Define data loaders with the current batch size\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Train the model\n",
        "    train_network(net, train_loader, test_loader, epochs=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TR6l4_584fL"
      },
      "source": [
        "### 2.3.2. How does learning rate work?\n",
        "\n",
        "When you are trying to train a neural network it is really hard to choose a proper learning rate.\n",
        "\n",
        "Try to train your model with different learning rates and plot the training accuracy, test accuracy and loss and compare the training progress for learning rates = 10, 0.1, 0.01, 0.0001.\n",
        "\n",
        "Analyze the results and choose the best one. Why did you choose this value?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7V03_lnCgRm"
      },
      "source": [
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define learning rates\n",
        "learning_rates = [10, 0.1, 0.01, 0.0001]\n",
        "\n",
        "# Lists to store training progress\n",
        "train_losses_list = []\n",
        "train_accuracies_list = []\n",
        "test_accuracies_list = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    # Initialize CoolNet\n",
        "    net = CoolNet()\n",
        "\n",
        "    # Define optimizer with current learning rate\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "    # Train the model\n",
        "    train_losses, train_accuracies, test_accuracies = train_network(net, optimizer, train_loader, test_loader, epochs=10)\n",
        "\n",
        "    # Store training progress\n",
        "    train_losses_list.append(train_losses)\n",
        "    train_accuracies_list.append(train_accuracies)\n",
        "    test_accuracies_list.append(test_accuracies)\n",
        "\n",
        "# Plot training progress for each learning rate\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot training and test losses\n",
        "plt.subplot(2, 1, 1)\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    plt.plot(train_losses_list[i], label=f'LR={lr}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and test accuracies\n",
        "plt.subplot(2, 1, 2)\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    plt.plot(train_accuracies_list[i], label=f'Train LR={lr}')\n",
        "    plt.plot(test_accuracies_list[i], label=f'Test LR={lr}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Test Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6BXXhbuPMt7"
      },
      "source": [
        "**Question**:\n",
        "Analyze the results and choose one value to use. Why did you choose this value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG1NBqRP3HPM"
      },
      "source": [
        "After analyzing the training progress with different learning rates, I choose a learning rate of 0.01. This choice is based on several factors. Firstly, a learning rate of 0.01 resulted in stable training, with the loss decreasing gradually without oscillations or divergences. This stability ensures consistent improvement in model performance over epochs. Secondly, the model trained with a learning rate of 0.01 exhibited steady convergence, with both training and test accuracies improving consistently. This indicates effective learning and generalization capabilities of the model. Additionally, a learning rate of 0.01 strikes a good balance between stability and convergence speed. It allows the model to learn efficiently without risking divergence or slow convergence, making it suitable for training the CoolNet model on the CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdbH0yiXMgda"
      },
      "source": [
        "### 2.3.3. Learning rate scheduling\n",
        "During training it is often useful to reduce learning rate as the training progresses.\n",
        "\n",
        "Fill in `set_learning_rate` below to scale the learning rate by 0.1 (reduce by 90%) every 30 epochs and observe the behavior of network for 90 epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxYH7HZOM397"
      },
      "source": [
        "def set_learning_rate(optimizer, epoch, base_lr):\n",
        "    lr = base_lr * (0.1 ** (epoch // 30))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpx6QCf-McwG"
      },
      "source": [
        "net = CoolNet()\n",
        "train_losses, train_accuracies, test_accuracies = train_network(\n",
        "    net,\n",
        "    lr_func=set_learning_rate,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    lr=0.01,\n",
        "    epochs=90,\n",
        "    batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk20p44TPCdn"
      },
      "source": [
        "**Question**:\n",
        "What do you observe? Why do you think it is useful to decrease the learning rate over time?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lngfOx-33J6N"
      },
      "source": [
        "\n",
        "Observation:\n",
        "As the training progresses with a learning rate scheduling strategy of decreasing the learning rate by 90% every 30 epochs, several observations can be made:\n",
        "\n",
        "Initially, the model makes large updates to its parameters due to the higher learning rate.\n",
        "As the learning rate decreases over time, the updates become smaller, allowing the model to fine-tune its parameters more delicately.\n",
        "This gradual reduction in learning rate helps prevent overshooting the optimal parameter values and allows the model to converge more smoothly towards the global minimum of the loss function.\n",
        "Importance of Decreasing Learning Rate:\n",
        "Decreasing the learning rate over time is crucial for several reasons:\n",
        "\n",
        "Stability: It helps stabilize the training process by preventing the model from making overly large updates to its parameters, which can cause instability or divergence.\n",
        "Convergence: Gradually reducing the learning rate allows the optimization process to settle into narrower and deeper areas of the loss landscape, facilitating convergence towards a more optimal solution.\n",
        "Fine-tuning: Lowering the learning rate enables the model to fine-tune its parameters more precisely, resulting in improved generalization performance on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an3_bp-d84fM"
      },
      "source": [
        "### 2.3.4. Data Augmentation\n",
        "\n",
        "Most of the popular computer vision datasets have tens of thousands of images.\n",
        "\n",
        "Cifar-10 is a dataset of 60000 32x32 colour images in 10 classes, which can be relatively small in compare to ImageNet which has 1M images.\n",
        "\n",
        "The more the number of parameters is, the more likely our model is to overfit to the small dataset.\n",
        "As you might have already faced this issue while training the CoolNet, after some iterations the training accuracy reaches its maximum (saturates) while the test accuracy is still relatively low.\n",
        "\n",
        "To solve this problem, we use the data augmentation to help the network avoid overfitting.\n",
        "\n",
        "Add data transformations in to the class below and compare the results. You are free to use any type and any number of data augmentation techniques.\n",
        "\n",
        "Just be aware that data augmentation should just happen during training phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNRQ5fhT84fN"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define data augmentation transformations for training data\n",
        "train_transform = transforms.Compose([\n",
        "    # Randomly apply horizontal flips with a probability of 0.5\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Randomly apply vertical flips with a probability of 0.5\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    # Randomly rotate the image by a maximum of 10 degrees\n",
        "    transforms.RandomRotation(10),\n",
        "    # Randomly adjust brightness and contrast\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0, hue=0),\n",
        "    # Randomly apply affine transformations such as scaling, shearing, and translation\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=0.1),\n",
        "    # Convert the image to a PyTorch tensor\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the image data\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6ntBrXZ84fP"
      },
      "source": [
        "net = CoolNet()\n",
        "train_losses, train_accuracies, test_accuracies = train_network(\n",
        "    net,\n",
        "    criterion=nn.CrossEntropyLoss(),\n",
        "    train_transforms=train_transform,\n",
        "    lr=0.01,\n",
        "    epochs=30,\n",
        "    batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QENsIio684fR"
      },
      "source": [
        "plot_results(train_losses, train_accuracies, test_accuracies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9JVV5AX84fV"
      },
      "source": [
        "**Question**: How does the model trained with data augmentation compared to the model trained without?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znxg2Yh03MaX"
      },
      "source": [
        "Training the model with data augmentation often yields superior generalization compared to training without it. Without data augmentation, the model may overfit to the training data, memorizing specific patterns rather than learning generalizable features. This leads to high training accuracy but poor performance on unseen data. Data augmentation introduces variations to the training data, such as rotations, flips, and crops, diversifying the training examples. By exposing the model to a broader range of variations, data augmentation encourages the model to learn more robust and invariant representations. Consequently, the model becomes better equipped to handle variations in real-world data, resulting in improved performance on unseen examples. Data augmentation acts as a form of regularization, helping prevent overfitting by providing a more comprehensive and diverse training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUnfgA5G84fX"
      },
      "source": [
        "### 2.3.5. Change the loss function\n",
        "\n",
        "Try Mean Squared Error loss instead of Cross Entropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfAh2MwQ84fY"
      },
      "source": [
        "class MSELossClassification(nn.Module):\n",
        "  def forward(self, output, labels):\n",
        "    one_hot_encoded_labels = \\\n",
        "      torch.nn.functional.one_hot(labels, num_classes=output.shape[1]).float()\n",
        "    return nn.functional.mse_loss(output, one_hot_encoded_labels)\n",
        "\n",
        "net = CoolNet()\n",
        "train_losses, train_accuracies, test_accuracies = train_network(\n",
        "    net,\n",
        "    criterion=MSELossClassification(),\n",
        "    lr=0.01,\n",
        "    epochs=50,\n",
        "    batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TePejy2984fb"
      },
      "source": [
        "plot_results(train_losses, train_accuracies, test_accuracies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDVB6Jl984fe"
      },
      "source": [
        "**Question**:\n",
        "How does this affects the results? Explain why you think this is happening."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXt3dmW13SAa"
      },
      "source": [
        "Replacing Cross Entropy loss with Mean Squared Error (MSE) for classification tasks often leads to inferior results. MSE loss, designed for regression tasks, penalizes deviations between predicted probabilities and one-hot encoded target labels, which may not align well with the classification task's objective. This mismatch can hinder the model's ability to learn meaningful representations and correlations between features and target classes. Additionally, MSE loss doesn't account for the probabilistic nature of classification, leading to less calibrated probability estimates and suboptimal convergence. Cross Entropy loss, tailored for classification, encourages the model to produce well-calibrated probabilities, making it more suitable for classification tasks and yielding better performance overall."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "AiFFPLi0jn_n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaH9ibDt_EJk"
      },
      "source": [
        "## Turning In\n",
        "\n",
        "You're done! You just need to turn in the notebook file.\n",
        "\n",
        "Go to `File > Download .ipynb` and download the file as `lab2.ipynb`. Turn in only this file.\n",
        "\n",
        "Make sure that you've answered all questions and all plots are correct."
      ]
    }
  ]
}